## Chapter 6:深度前馈网络

前馈神经网络之所以被称为网络，是因为他们经常用许多不同的函数复合在一起表示。

假如有三个函数
$$
f^{(1)}, f^{(2)}, f^{(3)}
$$
连接在一个链上形成以下形式
$$
f(x)=f^{(3)}(f^{(2)}(f^{(1))})))
$$

$$
f^{(1)}称为网络的第一层，f^{(2)}称为网络的第二层
$$

链的全长称为网络的深度，前馈网络的最后一层称为输出层，隐藏层的维数决定了模型的宽度。

#### 6.1 实例：学习XOR

整流线性激活函数（大多数前馈神经网络的默认激活函数）：
$$
g(z)=\max \left \{ 0, z \right \}
$$
输入信号小于0时，输出就是0.大于0的情况下，输出等于输入。

#### 6.2 基于梯度的学习

线性模型和神经网络的最大区别：神经网络的非线性导致大多数我们感兴趣的代价函数变得非凸。凸优化从任何一种初始参数出发都会收敛，用于非凸损失函数的随机梯度下降算法没有这种收敛性保证，并且对参数的初始值也很敏感。

计算梯度对于神经网络会稍微复杂一些，但是仍然可以很高效而精确的实现。

##### 6.2.1 代价函数

可以使用训练数据和模型预测间的交叉熵（Cross Entropy）来作为代价函数。

什么是交叉熵？

假设有两个分布p和q，则它们在给定样本集上的交叉熵定义如下：
$$
CEH(p,q)=E_{p}[-\log q]=-\sum_{x\epsilon X} p(x)\log q(x)
\\p：真实样本分布，服从参数为p的0-1分布，即X-B(1,p)
\\q:  待估计的模型，服从参数为q的0-1分布，即X-B(1,q)
$$

###### 6.2.1.1 使用最大似然学习条件分布

大多数现代的神经网络使用最大似然来训练，它与训练数据和模型分布间的交叉熵等价。

优势：使用最大似然来导出代价函数的方法的优势是，它减轻了为每个模型设计代价函数的负担。

##### 6.2.2 输出单元

###### 6.2.2.2 用于Bornoulli输出分布的sigmoid单元

分对数（logit）：定义这种二值型变量分布的变量Z就是LOGIT。
$$
\log P(y)=yz
\\P(y)=\exp (yz)
\\P(y)=\frac{\exp (yz)}{\sum_{1}^{y=0}}\exp (yz)
$$

###### 6.2.2.3 用于Multinoulli输出分布的softmax单元

当我们想要表示一个具有n个可能取值的离散型随机变量的分布时，都可以使用softmax函数。其中sigmoid函数用来表示二值型变量的分布。

为了推广到具有n个值得离散型变量的情况，现在需要创造一个向量
$$
\widehat{y}
$$
它的每个元素是
$$
\widehat{y}_{i}=P(y=i\mid x)
$$
我们不仅要求每个
$$
\widehat{y}_{i}
$$
元素介于0和1之间，还要使得整个向量的和为1。

最终softmax函数的形式为：
$$
\log softmax(z)_{i}=\exp (z_{i})\mid \sum_{j}^{} \exp(z_{j})
$$

#### 6.3 隐藏单元

大多数的隐藏单元都可以描述为接受输入向量x，计算仿射变换
$$
z=W^{T}x+b
$$
，然后使用一个逐元素的非线性函数g(z)。大多数隐藏单元的区别仅仅在于激活函数g(z)的形式。

##### 6.3.1 整流线性单元

整流线性单元的3个扩展基于当
$$
z_{i} < 0
$$
时，使用一个非零的斜率
$$
\alpha _{i}:h_{i}=g(z,\alpha )_{i}=max(0,z_{i}) + \alpha _{i}min(0, z_{i})
$$
绝对值整流固定
$$
\alpha _{i}=-1
$$
来得到
$$
g(z)=\mid z  \mid
$$

#### 6.5 反向传播

输入提供初始信息，然乎传播到每一层的隐藏单元，最终给产生输出。称之为前向传播。

> http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html